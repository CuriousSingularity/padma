# Training configuration
epochs: 100
batch_size: 32
accumulation_steps: 1  # Gradient accumulation steps

# Optimizer configuration
optimizer:
  name: adamw  # adam, adamw, sgd
  lr: 1e-4
  weight_decay: 0.01
  momentum: 0.9  # Only for SGD
  betas: [0.9, 0.999]  # Only for Adam/AdamW

# Learning rate scheduler
scheduler:
  name: cosine  # cosine, step, plateau, onecycle
  warmup_epochs: 5
  min_lr: 1e-6
  # For step scheduler
  step_size: 30
  gamma: 0.1
  # For plateau scheduler
  patience: 10
  factor: 0.1

# Loss function
loss:
  name: cross_entropy  # cross_entropy, label_smoothing
  label_smoothing: 0.0

# Early stopping
early_stopping:
  enabled: false
  patience: 20
  monitor_metric: val_loss
  monitor_mode: min
